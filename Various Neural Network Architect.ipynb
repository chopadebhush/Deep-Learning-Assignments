{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "#### 1. Describe the basic structure of a Feedforward Neural Network (FNN). What is the purpose of the  activation function? ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Ans :\n\nA Feedforward Neural Network (FNN) is one of the simplest types of artificial neural networks. It consists of three main layers:\n\nInput Layer: Receives the input data.\nHidden Layers: Intermediate layers where computations are performed on the inputs. An FNN can have multiple hidden layers, each comprising multiple neurons.\nOutput Layer: Produces the final result or prediction of the network based on the computations in the hidden layers.\nIn an FNN, data flows in one direction, from the input layer through the hidden layers to the output layer, without looping back.\n\nPurpose of Activation Function: The activation function introduces non-linearity into the network, allowing it to model complex relationships in the data. Without non-linear activation functions, an FNN would only be capable of learning linear mappings, limiting its performance on more complex tasks. Common activation functions include:\n\nReLU (Rectified Linear Unit): Helps in mitigating vanishing gradients.\nSigmoid: Useful for binary classification tasks.\nTanh: A zero-centered activation function.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "#### 2 Explain the role of convolutional layers in a CNN. Why are pooling layers commonly used, and what do  they achieve? ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Ans :\n\nConvolutional Layers in a Convolutional Neural Network (CNN) are responsible for detecting features (like edges, textures, and shapes) in input images by applying convolution operations using small filters or kernels. Each filter \"slides\" across the image, capturing local patterns and producing feature maps. The role of the convolutional layer is to:\n\nAutomatically learn spatial hierarchies of patterns.\nReduce the number of parameters compared to fully connected layers.\nPooling Layers: Pooling layers follow convolutional layers and are used to reduce the spatial dimensions (width and height) of feature maps, while retaining important features. The key benefits of pooling layers include:\n\nDimensionality Reduction: Decreases the computational load by reducing the number of parameters.\nTranslation Invariance: Helps the network become more robust to small movements or translations of objects in the image.\nCommon Pooling Types:\n\nMax Pooling: Takes the maximum value from each region in the feature map.\nAverage Pooling: Takes the average value from each region.\nMax pooling is more common as it preserves strong features.\n\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "\n#### 3 What is the key characteristic that differentiates Recurrent Neural Networks (RNNs) from other neural  networks? How does an RNN handle sequential data? ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Ans :\n\nThe key characteristic that differentiates Recurrent Neural Networks (RNNs) from other neural networks is their ability to process sequential data. Unlike FNNs, RNNs have connections that form directed cycles, allowing information to persist. They maintain a hidden state that is updated at each time step, enabling them to retain information from previous time steps and use it to influence current and future outputs.\n\nHandling Sequential Data:\n\nMemory: RNNs are ideal for tasks like time series forecasting, language modeling, and speech recognition, where the current input depends on previous inputs.\nBackpropagation Through Time (BPTT): RNNs apply backpropagation through multiple time steps to adjust the weights based on the entire sequence.\nHowever, RNNs face challenges such as the vanishing gradient problem, which limits their ability to capture long-term dependencies.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "#### 4 . Discuss the components of a Long Short-Term Memory (LSTM) network. How does it address the  vanishing gradient problem? ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Ans:\n\nLSTM Networks are a special kind of RNN that are designed to better capture long-term dependencies and overcome the vanishing gradient problem. LSTMs have a more complex architecture that includes memory cells and gates to control the flow of information.\n\nKey Components:\n\nMemory Cell: The core unit that stores information across time steps.\nForget Gate: Decides which information from the previous state should be forgotten.\nInput Gate: Decides which new information should be stored in the memory cell.\nOutput Gate: Decides what information should be output at each time step.\nVanishing Gradient Problem: In standard RNNs, as gradients are propagated through many time steps, they tend to shrink, making it difficult to learn long-range dependencies. LSTMs address this by allowing gradients to flow more easily through time via memory cells, mitigating the vanishing gradient problem. The architecture ensures that important information is retained and irrelevant information is discarded efficiently.\n\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "#### 5 Describe the roles of the generator and discriminator in a Generative Adversarial Network (GAN). What is  the training objective for each?",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Ans :\n\nIn a Generative Adversarial Network (GAN), two networks—the generator and the discriminator—are trained simultaneously in a competitive framework.\n\nGenerator:\n\nRole: The generator's job is to produce fake data (e.g., images) that resemble the real data. It takes random noise as input and generates synthetic samples.\nTraining Objective: The generator tries to fool the discriminator into thinking that the generated data is real. It aims to minimize the loss by producing more realistic samples over time.\nDiscriminator:\n\nRole: The discriminator distinguishes between real data (from the training dataset) and fake data (produced by the generator).\nTraining Objective: The discriminator's goal is to maximize the probability of correctly identifying real versus fake data. It aims to minimize its loss by improving its classification accuracy.\nThe training objective of a GAN is a minimax game:\n\nThe generator tries to minimize the discriminator’s ability to distinguish real from fake data.\nThe discriminator tries to maximize its accuracy in identifying real versus fake data.\n\n\n\n\n\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}